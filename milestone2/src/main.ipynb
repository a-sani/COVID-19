{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dd9b7ae739fe05fd14e489c593dddcdef34b4828e46eb743d56b94c2d60eb567"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "***NOTE: The entire notebook takes ~30 minutes to complete. This is mainly because of the accuracy curves created for each model to detect overfitting (each accuracy curve loops through a range of values of a parameter that it trains the model on).***\n",
    "\n",
    "***So if you want to avoid running the code for plotting the accuracy curve, please skip over the \"Detecting Overfitting\" sections (1.3, 2.3, and 3.3) to save time. The accuracy_curve graphs are already saved in the plots folder.***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "***Import all the libraries***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "source": [
    "***Add the dataset and convert the date format to integer type.***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/cases_train_processed.csv')\n",
    "\n",
    "# Some preprocessing\n",
    "df[\"date_confirmation\"] = pd.to_datetime(df[\"date_confirmation\"]).dt.strftime(\"%Y%m%d\").astype(int) # convert date from object type to int type\n",
    "df2 = df.copy() # creating a copy for lightgbm because of different processing method"
   ]
  },
  {
   "source": [
    "# 1. K-Nearest Neighbours Classifier\n",
    "### 1.1 Building the model\n",
    "*Step 1: Convert all the categorical features into numerical by using Label encoding.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use label encoder to normalize categorical features in dataframe\n",
    "le = LabelEncoder()\n",
    "categoricalFeatures = ['sex', 'province', 'country','key','additional_information', 'source']\n",
    "for feat in categoricalFeatures:\n",
    "    df[feat]= le.fit_transform(df[feat])"
   ]
  },
  {
   "source": [
    "*Split the dataframe into the training data and validation data after separating the outcomes column from the rest of the dataset.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['outcome']\n",
    "X = df.drop(['outcome'], axis=1)\n",
    "\n",
    "#split data into training and validation sets\n",
    "training_data, validation_data, training_truth, validation_truth = train_test_split(X, y, train_size=0.8, test_size=0.20, random_state=11)"
   ]
  },
  {
   "source": [
    "*Create and fit the K-Nearest Neighbours Classifier and save it as a pickle file.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 9, weights = 'distance')\n",
    "knn.fit(training_data, training_truth)\n",
    "# save the trained model to disk\n",
    "pickle.dump(knn, open('../models/knn_classifier.pkl', 'wb'))"
   ]
  },
  {
   "source": [
    "*Then, load the model from the pickle file and find the predictions on the training and validation data from the model.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "knn = pickle.load(open('../models/knn_classifier.pkl', 'rb'))\n",
    "# predict on the training data\n",
    "training_prediction = knn.predict(training_data)\n",
    "#predict on the validation data\n",
    "validation_prediction = knn.predict(validation_data)"
   ]
  },
  {
   "source": [
    "### 1.2 Evaluating the model\n",
    "*The metrics used to evaluate the model are the Accuracy score, Precision, Recall, F1-score, and the support count.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,validation_prediction)\n",
    "print(\"K-Nearest Neighbours Model Predictions:\\n\")\n",
    "\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)"
   ]
  },
  {
   "source": [
    "*Confusion matrix to provide a better insight to the predictions made by the KNN classifier.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 15), nrows = 1, ncols = 2) \n",
    "metrics.plot_confusion_matrix(knn, training_data, training_truth, cmap = plt.cm.Blues, ax = ax[0], values_format = '.6g') \n",
    "ax[0].set_title('K Neighbours - Confusion Matrix of Training Data')\n",
    "metrics.plot_confusion_matrix(knn, validation_data, validation_truth, cmap = plt.cm.Blues, ax = ax[1], values_format = '.6g') \n",
    "ax[1].set_title('K Neighbours - Confusion Matrix of Validation Data')\n",
    "\n",
    "# figure settings\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.4)\n",
    "fig.subplots_adjust(right=0.9)\n",
    "fig.savefig('../plots/knn_cm.png', bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "source": [
    "### 1.3 Detecting Overfitting\n",
    "*Plotting the Accuracy measure on a range of the \"n_neighbours\" parameter to detect if the classifier is overfitting.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = [1,10,20,30,40,50,60]\n",
    "train_accuracy = np.empty(len(neighbors)) \n",
    "test_accuracy = np.empty(len(neighbors)) \n",
    "  \n",
    "# Loop over K values \n",
    "for i, k in enumerate(neighbors): \n",
    "    knn = KNeighborsClassifier(n_neighbors=k,weights='distance') \n",
    "    knn.fit(training_data, training_truth) \n",
    "      \n",
    "    # Compute traning and test data accuracy \n",
    "    train_accuracy[i] = knn.score(training_data, training_truth) \n",
    "    test_accuracy[i] = knn.score(validation_data, validation_truth) \n",
    "  \n",
    "# Generate plot \n",
    "plt.plot(neighbors, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy') \n",
    "plt.title('Accuracy measure of KNN according to n_neighbours parameter')\n",
    "  \n",
    "plt.legend() \n",
    "plt.xlabel('n_neighbours') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig('../plots/knn_accuracy_curve.png')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "# 2. Random Forests Classifier\n",
    "### 2.1 Building the model\n",
    "*Using the same training and validation dataset split from KNN, the Random Forests classifier is built and saved as a pickle.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_clf = RandomForestClassifier(max_depth = 22)\n",
    "RF_clf.fit(training_data, training_truth)\n",
    "# save the trained model to disk\n",
    "pickle.dump(RF_clf, open('../models/rf_classifier.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "RF_clf = pickle.load(open('../models/rf_classifier.pkl', 'rb'))\n",
    "\n",
    "# prediction on validation and training set\n",
    "RF_validation_prediction = RF_clf.predict(validation_data)\n",
    "RF_training_prediction = RF_clf.predict(training_data)\n"
   ]
  },
  {
   "source": [
    "### 2.2 Evaluating the model\n",
    "*The metrics used to evaluate the model are the Accuracy score, Precision, Recall, F1-score, and the support count.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = metrics.accuracy_score(RF_training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,RF_training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(RF_validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,RF_validation_prediction)\n",
    "print(\"Random Forests Model Predictions:\\n\")\n",
    "\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)"
   ]
  },
  {
   "source": [
    "*Confusion matrix to provide a better insight to the predictions made by the Random Forests classifier.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 15), nrows = 1, ncols = 2) \n",
    "metrics.plot_confusion_matrix(RF_clf, training_data, training_truth, cmap = plt.cm.Blues, ax = ax[0], values_format = '.6g') \n",
    "ax[0].set_title('Random Forest - Confusion Matrix of Training Data')\n",
    "metrics.plot_confusion_matrix(RF_clf, validation_data, validation_truth, cmap = plt.cm.Blues, ax = ax[1], values_format = '.6g') \n",
    "ax[1].set_title('Random Forest - Confusion Matrix of Validation Data')\n",
    "\n",
    "# figure settings\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.4)\n",
    "fig.subplots_adjust(right=0.9)\n",
    "fig.savefig('../plots/rf_cm.png', bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "source": [
    "### 2.3 Detecting Overfitting\n",
    "*Plotting the Accuracy measure on a range of the \"max_depth\" parameter to detect if the classifier is overfitting.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_max_depth = np.arange(1, 31) \n",
    "RF_train_accuracy = np.empty(len(RF_max_depth)) \n",
    "RF_test_accuracy = np.empty(len(RF_max_depth)) \n",
    "\n",
    "for i, depth in enumerate(RF_max_depth):\n",
    "    \n",
    "    RF_classifier = RandomForestClassifier(max_depth = depth)\n",
    "    RF_classifier.fit(training_data, training_truth)\n",
    "\n",
    "    RF_validation_pred = RF_classifier.predict(validation_data)\n",
    "    RF_training_pred = RF_classifier.predict(training_data)\n",
    "    \n",
    "    RF_train_accuracy[i] = metrics.accuracy_score(RF_training_pred, training_truth)\n",
    "    RF_test_accuracy[i] = metrics.accuracy_score(RF_validation_pred, validation_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot \n",
    "plt.plot(RF_max_depth, RF_test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(RF_max_depth, RF_train_accuracy, label = 'Training dataset Accuracy') \n",
    "  \n",
    "plt.legend(loc = 'lower right') \n",
    "plt.title('Accuracy measure of Random Forests based on max_depth')\n",
    "plt.xlabel('Tree Max Depth') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig('../plots/rf_accuracy_curve.png')\n",
    "plt.show() "
   ]
  },
  {
   "source": [
    "# 3. LightGBM Classifier\n",
    "### 3.1 Building the model\n",
    "*First, convert all the categorical features into the category type which is used by LightGBM for processing categorical data.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in df2.columns:\n",
    "    col_type = df2[c].dtype\n",
    "    if col_type == 'object' or col_type.name == 'category':\n",
    "        df2[c] = df2[c].astype('category')"
   ]
  },
  {
   "source": [
    "*Split the dataframe into the training data and validation data after separating the outcomes column from the rest of the dataset.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df2['outcome']\n",
    "X = df2.drop(['outcome'], axis=1)\n",
    "training_data, validation_data, training_truth, validation_truth = train_test_split(X, y, train_size=0.8, test_size=0.20, random_state=11)"
   ]
  },
  {
   "source": [
    "*Create and fit the LightGBM Classifier and save it as a pickle file.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model on the training dataset\n",
    "lgbm_model = lgb.LGBMClassifier(boosting_type='gbdt',num_leaves = 100) #Chose 100 as the optimal # of leaves after looking at the accuracy curve \n",
    "fit_params={'feature_name': 'auto', 'categorical_feature': 'auto'}\n",
    "lgbm_model.fit(training_data, training_truth, **fit_params)\n",
    "# save the trained model to disk\n",
    "pickle.dump(lgbm_model, open('../models/lgbm_classifier.pkl', 'wb'))\n"
   ]
  },
  {
   "source": [
    "*Then, load the model from the pickle file and find the predictions on the training and validation data from the model.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "lgbm_model = pickle.load(open('../models/lgbm_classifier.pkl', 'rb'))\n",
    "# predict on the training data\n",
    "training_prediction = lgbm_model.predict(training_data)\n",
    "#predict on the validation data\n",
    "validation_prediction = lgbm_model.predict(validation_data)"
   ]
  },
  {
   "source": [
    "### 3.2 Evaluating the model\n",
    "*The metrics used to evaluate the model are the Accuracy score, Precision, Recall, F1-score, and the support count.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,validation_prediction)\n",
    "print(\"LightGBM Model Predictions:\\n\")\n",
    "\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)"
   ]
  },
  {
   "source": [
    "*Also created a confusion matrix to provide a good insight to the predictions.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 15), nrows = 1, ncols = 2) \n",
    "metrics.plot_confusion_matrix(lgbm_model, training_data, training_truth, cmap = plt.cm.Blues, ax = ax[0], values_format = '.6g') \n",
    "ax[0].set_title('LightGBM - Confusion Matrix of Training Data')\n",
    "metrics.plot_confusion_matrix(lgbm_model, validation_data, validation_truth, cmap = plt.cm.Blues, ax = ax[1], values_format = '.6g') \n",
    "ax[1].set_title('LightGBM - Confusion Matrix of Validation Data')\n",
    "\n",
    "# figure settings\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.4)\n",
    "fig.subplots_adjust(right=0.9)\n",
    "fig.savefig('../plots/lgbm_cm.png', bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "source": [
    "### 3.3 Detecting Overfitting\n",
    "*Plotting the Accuracy measure on a range of the \"num_leaves\" parameter to detect if the classifier is overfitting.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "leaves = [20,40,60,80,100,120,160,200,240,300]\n",
    "train_accuracy = np.empty(len(leaves)) \n",
    "test_accuracy = np.empty(len(leaves)) \n",
    "  \n",
    "# Loop over K values \n",
    "for index, value in enumerate(leaves): \n",
    "    lgbm_model = lgb.LGBMClassifier(num_leaves=value)\n",
    "    lgbm_model.fit(training_data, training_truth, **fit_params)\n",
    "\n",
    "    training_prediction = lgbm_model.predict(training_data)\n",
    "    validation_prediction = lgbm_model.predict(validation_data)\n",
    "    train_accuracy[index] = metrics.accuracy_score(training_prediction, training_truth)\n",
    "    test_accuracy[index] = metrics.accuracy_score(validation_prediction, validation_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate plot \n",
    "plt.plot(leaves, test_accuracy, label = 'Testing dataset Accuracy') \n",
    "plt.plot(leaves, train_accuracy, label = 'Training dataset Accuracy') \n",
    "  \n",
    "plt.legend() \n",
    "plt.title('Accuracy measure of LightGBM according to num_leaves parameter')\n",
    "plt.xlabel('Number of leaves') \n",
    "plt.ylabel('Accuracy') \n",
    "plt.savefig('../plots/lgbm_accuracy_curve.png')\n",
    "plt.show() "
   ]
  },
  {
   "source": [
    "***From what we analyzed by observing the evaluation metrics (confusion matrix & support metric), the class labels of outcome are imbalanced. This could be a major contributing factor in misclassification, and this is something we hope to fix in the next milestone.***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}