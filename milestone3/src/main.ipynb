{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0dd9b7ae739fe05fd14e489c593dddcdef34b4828e46eb743d56b94c2d60eb567",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "***Import all the libraries***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN\n"
   ]
  },
  {
   "source": [
    "***Add the dataset and convert the date format to integer type.***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/cases_train_processed.csv')\n",
    "test_df = pd.read_csv('../data/cases_test_processed.csv')\n",
    "\n",
    "# Some preprocessing\n",
    "# convert date from object type to int type\n",
    "train_df[\"date_confirmation\"] = pd.to_datetime(train_df[\"date_confirmation\"]).dt.strftime(\"%Y%m%d\").astype(int) \n",
    "test_df[\"date_confirmation\"] = pd.to_datetime(test_df[\"date_confirmation\"]).dt.strftime(\"%Y%m%d\").astype(int) \n",
    "# train_df2 = train_df.copy() # creating a copy for lightgbm because of different processing method\n",
    "# test_df2 = test_df.copy() # creating a copy for lightgbm because of different processing method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use label encoder to normalize categorical features in dataframe\n",
    "le = LabelEncoder()\n",
    "categoricalFeatures = ['sex', 'province', 'country','key','additional_information', 'source']\n",
    "for feat in categoricalFeatures:\n",
    "    train_df[feat]= le.fit_transform(train_df[feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "nonhospitalized    149990\n",
       "hospitalized       125000\n",
       "recovered           88137\n",
       "deceased             4499\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "train_df[\"outcome\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['outcome']\n",
    "X = train_df.drop(['outcome'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training and validation sets\n",
    "training_data, validation_data, training_truth, validation_truth = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = SMOTE(random_state=0)\n",
    "training_data, training_truth = oversample.fit_resample(training_data, training_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "deceased           119928\n",
       "nonhospitalized    119928\n",
       "hospitalized       119928\n",
       "recovered          119928\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "training_truth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "nonhospitalized    30062\n",
       "hospitalized       24908\n",
       "recovered          17643\n",
       "deceased             913\n",
       "Name: outcome, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "validation_truth.value_counts()"
   ]
  },
  {
   "source": [
    "# 1. K-Nearest Neighbours Classifier\n",
    "### Testing with the oversampling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=9, weights='distance')"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 9, weights = 'distance')\n",
    "knn.fit(training_data, training_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_prediction = knn.predict(training_data)\n",
    "validation_prediction = knn.predict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "K-Nearest Neighbours Model Predictions:\n\nTRAINING\nAccuracy score: 0.78873\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.69      0.78      0.73    119928\n   hospitalized       0.74      0.68      0.71    119928\nnonhospitalized       1.00      1.00      1.00    119928\n      recovered       0.74      0.70      0.72    119928\n\n       accuracy                           0.79    479712\n      macro avg       0.79      0.79      0.79    479712\n   weighted avg       0.79      0.79      0.79    479712\n\n\nVALIDATION\nAccuracy score: 0.79084\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.05      0.49      0.09       913\n   hospitalized       0.85      0.66      0.74     24908\nnonhospitalized       0.99      0.99      0.99     30062\n      recovered       0.73      0.66      0.70     17643\n\n       accuracy                           0.79     73526\n      macro avg       0.66      0.70      0.63     73526\n   weighted avg       0.87      0.79      0.82     73526\n\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,validation_prediction)\n",
    "print(\"K-Nearest Neighbours Model Predictions:\\n\")\n",
    "\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)"
   ]
  },
  {
   "source": [
    "# 2. Random Forests Classifier\n",
    "### 2.1 Building the model\n",
    "*Using the same training and validation dataset split from KNN, the Random Forests classifier is built and saved as a pickle.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 3. LightGBM Classifier\n",
    "### 3.1 Building the model\n",
    "*First, convert all the categorical features into the category type which is used by LightGBM for processing categorical data.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "*Split the dataframe into the training data and validation data after separating the outcomes column from the rest of the dataset.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'min_data_in_leaf': [20, 30, 50, 100, 300]\n",
    "\n",
    "def calc_deceasedRecall(truth, prediction):\n",
    "    return metrics.recall_score(truth,prediction, average=None)[0]\n",
    "\n",
    "def calc_deceasedPrecision(truth, prediction):\n",
    "    return metrics.precision_score(truth,prediction, average=None)[0]\n",
    "\n",
    "def calc_deceasedF1(truth, prediction):\n",
    "    return metrics.f1_score(truth,prediction, average=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scoring_metrics = {\n",
    "    'f1_deceased' : metrics.make_scorer(calc_deceasedF1),\n",
    "    'recall_deceased' : metrics.make_scorer(calc_deceasedRecall),\n",
    "    'overall_accuracy': metrics.make_scorer(metrics.accuracy_score),\n",
    "    'overall_recall': metrics.make_scorer(metrics.recall_score , average='macro'),\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'num_leaves': [60,90,120],\n",
    "    'n_estimators': [100,200,300],\n",
    "    # 'boosting_type': ['gbdt', 'dart', 'goss'],\n",
    "    'min_data_in_leaf': [60,80]\n",
    "    }\n",
    "    # took 1.5 hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[LightGBM] [Warning] min_data_in_leaf is set=60, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=60\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=LGBMClassifier(), n_jobs=-1,\n",
       "             param_grid={'min_data_in_leaf': [60, 80],\n",
       "                         'n_estimators': [100, 200, 300],\n",
       "                         'num_leaves': [60, 90, 120]},\n",
       "             refit='f1_deceased',\n",
       "             scoring={'f1_deceased': make_scorer(calc_deceasedF1),\n",
       "                      'overall_accuracy': make_scorer(accuracy_score),\n",
       "                      'overall_recall': make_scorer(recall_score, average=macro),\n",
       "                      'recall_deceased': make_scorer(calc_deceasedRecall)},\n",
       "             verbose=-1)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "lgb_model = lgb.LGBMClassifier()\n",
    "# lgb_paras = {'boosting_type': ['gbdt', 'dart', 'goss']}\n",
    "lgb_grid_search = GridSearchCV(lgb_model, param_grid=param_grid, scoring=scoring_metrics, cv=3, n_jobs=-1, refit='f1_deceased',verbose=-1)\n",
    "lgb_grid_search.fit(training_data, training_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the parameter variations and scores in a table\n",
    "lgbm_results = pd.DataFrame(lgb_grid_search.cv_results_)[['param_num_leaves', 'param_n_estimators', 'param_min_data_in_leaf','mean_test_f1_deceased', 'rank_test_f1_deceased','mean_test_recall_deceased', 'mean_test_overall_accuracy','mean_test_overall_recall']]\n",
    "# best params: 'num_leaves' = 120,'n_estimators' = 300,'boosting_type' = 'gbdt','min_data_in_leaf' = 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   param_num_leaves param_n_estimators param_min_data_in_leaf  \\\n",
       "0                60                100                     60   \n",
       "1                90                100                     60   \n",
       "2               120                100                     60   \n",
       "3                60                200                     60   \n",
       "4                90                200                     60   \n",
       "5               120                200                     60   \n",
       "6                60                300                     60   \n",
       "7                90                300                     60   \n",
       "8               120                300                     60   \n",
       "9                60                100                     80   \n",
       "10               90                100                     80   \n",
       "11              120                100                     80   \n",
       "12               60                200                     80   \n",
       "13               90                200                     80   \n",
       "14              120                200                     80   \n",
       "15               60                300                     80   \n",
       "16               90                300                     80   \n",
       "17              120                300                     80   \n",
       "\n",
       "    mean_test_f1_deceased  rank_test_f1_deceased  mean_test_recall_deceased  \\\n",
       "0                0.736306                     17                   0.774540   \n",
       "1                0.748635                     15                   0.793109   \n",
       "2                0.756528                     11                   0.797737   \n",
       "3                0.756363                     12                   0.796495   \n",
       "4                0.761691                      9                   0.804341   \n",
       "5                0.764905                      5                   0.804816   \n",
       "6                0.761737                      8                   0.800188   \n",
       "7                0.766705                      3                   0.811996   \n",
       "8                0.769125                      1                   0.811045   \n",
       "9                0.735540                     18                   0.770095   \n",
       "10               0.747396                     16                   0.791133   \n",
       "11               0.754094                     14                   0.793376   \n",
       "12               0.755945                     13                   0.792634   \n",
       "13               0.760496                     10                   0.801147   \n",
       "14               0.764197                      6                   0.801539   \n",
       "15               0.761870                      7                   0.801831   \n",
       "16               0.766398                      4                   0.805558   \n",
       "17               0.768025                      2                   0.809019   \n",
       "\n",
       "    mean_test_overall_accuracy  mean_test_overall_recall  \n",
       "0                     0.781033                  0.781033  \n",
       "1                     0.789301                  0.789301  \n",
       "2                     0.794746                  0.794746  \n",
       "3                     0.794668                  0.794668  \n",
       "4                     0.799311                  0.799311  \n",
       "5                     0.802513                  0.802513  \n",
       "6                     0.799388                  0.799388  \n",
       "7                     0.803784                  0.803784  \n",
       "8                     0.805519                  0.805519  \n",
       "9                     0.779726                  0.779726  \n",
       "10                    0.788575                  0.788575  \n",
       "11                    0.793678                  0.793678  \n",
       "12                    0.794114                  0.794114  \n",
       "13                    0.798992                  0.798992  \n",
       "14                    0.801723                  0.801723  \n",
       "15                    0.799086                  0.799086  \n",
       "16                    0.803157                  0.803157  \n",
       "17                    0.804837                  0.804837  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>param_num_leaves</th>\n      <th>param_n_estimators</th>\n      <th>param_min_data_in_leaf</th>\n      <th>mean_test_f1_deceased</th>\n      <th>rank_test_f1_deceased</th>\n      <th>mean_test_recall_deceased</th>\n      <th>mean_test_overall_accuracy</th>\n      <th>mean_test_overall_recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60</td>\n      <td>100</td>\n      <td>60</td>\n      <td>0.736306</td>\n      <td>17</td>\n      <td>0.774540</td>\n      <td>0.781033</td>\n      <td>0.781033</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>90</td>\n      <td>100</td>\n      <td>60</td>\n      <td>0.748635</td>\n      <td>15</td>\n      <td>0.793109</td>\n      <td>0.789301</td>\n      <td>0.789301</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>120</td>\n      <td>100</td>\n      <td>60</td>\n      <td>0.756528</td>\n      <td>11</td>\n      <td>0.797737</td>\n      <td>0.794746</td>\n      <td>0.794746</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60</td>\n      <td>200</td>\n      <td>60</td>\n      <td>0.756363</td>\n      <td>12</td>\n      <td>0.796495</td>\n      <td>0.794668</td>\n      <td>0.794668</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>90</td>\n      <td>200</td>\n      <td>60</td>\n      <td>0.761691</td>\n      <td>9</td>\n      <td>0.804341</td>\n      <td>0.799311</td>\n      <td>0.799311</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>120</td>\n      <td>200</td>\n      <td>60</td>\n      <td>0.764905</td>\n      <td>5</td>\n      <td>0.804816</td>\n      <td>0.802513</td>\n      <td>0.802513</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>60</td>\n      <td>300</td>\n      <td>60</td>\n      <td>0.761737</td>\n      <td>8</td>\n      <td>0.800188</td>\n      <td>0.799388</td>\n      <td>0.799388</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>90</td>\n      <td>300</td>\n      <td>60</td>\n      <td>0.766705</td>\n      <td>3</td>\n      <td>0.811996</td>\n      <td>0.803784</td>\n      <td>0.803784</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>120</td>\n      <td>300</td>\n      <td>60</td>\n      <td>0.769125</td>\n      <td>1</td>\n      <td>0.811045</td>\n      <td>0.805519</td>\n      <td>0.805519</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>60</td>\n      <td>100</td>\n      <td>80</td>\n      <td>0.735540</td>\n      <td>18</td>\n      <td>0.770095</td>\n      <td>0.779726</td>\n      <td>0.779726</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>90</td>\n      <td>100</td>\n      <td>80</td>\n      <td>0.747396</td>\n      <td>16</td>\n      <td>0.791133</td>\n      <td>0.788575</td>\n      <td>0.788575</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>120</td>\n      <td>100</td>\n      <td>80</td>\n      <td>0.754094</td>\n      <td>14</td>\n      <td>0.793376</td>\n      <td>0.793678</td>\n      <td>0.793678</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>60</td>\n      <td>200</td>\n      <td>80</td>\n      <td>0.755945</td>\n      <td>13</td>\n      <td>0.792634</td>\n      <td>0.794114</td>\n      <td>0.794114</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>90</td>\n      <td>200</td>\n      <td>80</td>\n      <td>0.760496</td>\n      <td>10</td>\n      <td>0.801147</td>\n      <td>0.798992</td>\n      <td>0.798992</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>120</td>\n      <td>200</td>\n      <td>80</td>\n      <td>0.764197</td>\n      <td>6</td>\n      <td>0.801539</td>\n      <td>0.801723</td>\n      <td>0.801723</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>60</td>\n      <td>300</td>\n      <td>80</td>\n      <td>0.761870</td>\n      <td>7</td>\n      <td>0.801831</td>\n      <td>0.799086</td>\n      <td>0.799086</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>90</td>\n      <td>300</td>\n      <td>80</td>\n      <td>0.766398</td>\n      <td>4</td>\n      <td>0.805558</td>\n      <td>0.803157</td>\n      <td>0.803157</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>120</td>\n      <td>300</td>\n      <td>80</td>\n      <td>0.768025</td>\n      <td>2</td>\n      <td>0.809019</td>\n      <td>0.804837</td>\n      <td>0.804837</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "lgbm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a csv file\n",
    "lgbm_results.to_csv('../results/smote_lgbm.csv', index=False)"
   ]
  },
  {
   "source": [
    "*Create and fit the LightGBM Classifier and save it as a pickle file.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier(min_data_in_leaf=60, n_estimators=300, num_leaves=120)"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "source": [
    "# Fit the model on the training dataset\n",
    "lgbm_model = lgb.LGBMClassifier(num_leaves = 120, n_estimators=300,min_data_in_leaf=60)\n",
    "#fit_params={'feature_name': 'auto', 'categorical_feature': 'auto'}\n",
    "lgbm_model.fit(training_data, training_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LGBMClassifier()"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "# Fit the model on the training dataset\n",
    "lgbm_model = lgb.LGBMClassifier()\n",
    "lgbm_model.fit(training_data, training_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the training data\n",
    "training_prediction = lgbm_model.predict(training_data)\n",
    "#predict on the validation data\n",
    "validation_prediction = lgbm_model.predict(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth1)\n",
    "scores_training = metrics.classification_report(training_truth1,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth1)\n",
    "scores_validation = metrics.classification_report(validation_truth1,validation_prediction)\n",
    "print(\"LightGBM Model Predictions:\\n\")\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)\n",
    "\n",
    "# SCORES WITHOUT OVERSAMPLING \n",
    "# lgb.LGBMClassifier(boosting_type='gbdt',num_leaves = 120, n_estimators=300,min_data_in_leaf=60)"
   ]
  },
  {
   "source": [
    "### 3.2 Evaluating the model\n",
    "*The metrics used to evaluate the model are the Accuracy score, Precision, Recall, F1-score, and the support count.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Model Predictions:\n\nTRAINING\nAccuracy score: 0.82156\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.76      0.81      0.79    119928\n   hospitalized       0.79      0.71      0.75    119928\nnonhospitalized       1.00      1.00      1.00    119928\n      recovered       0.74      0.77      0.75    119928\n\n       accuracy                           0.82    479712\n      macro avg       0.82      0.82      0.82    479712\n   weighted avg       0.82      0.82      0.82    479712\n\n\nVALIDATION\nAccuracy score: 0.82590\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.07      0.43      0.12       913\n   hospitalized       0.86      0.71      0.78     24908\nnonhospitalized       0.99      0.99      0.99     30062\n      recovered       0.74      0.73      0.74     17643\n\n       accuracy                           0.83     73526\n      macro avg       0.67      0.72      0.66     73526\n   weighted avg       0.88      0.83      0.85     73526\n\n"
     ]
    }
   ],
   "source": [
    "# SMOTE\n",
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,validation_prediction)\n",
    "print(\"LightGBM Model Predictions:\\n\")\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)\n",
    "# lgb.LGBMClassifier(num_leaves = 120, n_estimators=300,min_data_in_leaf=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LightGBM Model Predictions:\n\nTRAINING\nAccuracy score: 0.77084\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.70      0.75      0.72    119928\n   hospitalized       0.73      0.64      0.68    119928\nnonhospitalized       0.99      0.99      0.99    119928\n      recovered       0.67      0.70      0.69    119928\n\n       accuracy                           0.77    479712\n      macro avg       0.77      0.77      0.77    479712\n   weighted avg       0.77      0.77      0.77    479712\n\n\nVALIDATION\nAccuracy score: 0.79466\nClassification report: \n                  precision    recall  f1-score   support\n\n       deceased       0.07      0.50      0.12       913\n   hospitalized       0.84      0.65      0.74     24908\nnonhospitalized       0.99      0.99      0.99     30062\n      recovered       0.69      0.68      0.69     17643\n\n       accuracy                           0.79     73526\n      macro avg       0.65      0.71      0.63     73526\n   weighted avg       0.86      0.79      0.82     73526\n\n"
     ]
    }
   ],
   "source": [
    "training_accuracy = metrics.accuracy_score(training_prediction, training_truth)\n",
    "scores_training = metrics.classification_report(training_truth,training_prediction)\n",
    "validation_accuracy = metrics.accuracy_score(validation_prediction, validation_truth)\n",
    "scores_validation = metrics.classification_report(validation_truth,validation_prediction)\n",
    "print(\"LightGBM Model Predictions:\\n\")\n",
    "print('TRAINING\\nAccuracy score: {0:0.5f}'.format(training_accuracy))\n",
    "print('Classification report: \\n',scores_training)\n",
    "print('\\nVALIDATION\\nAccuracy score: {0:0.5f}'.format(validation_accuracy))\n",
    "print('Classification report: \\n',scores_validation)\n",
    "# lgb.LGBMClassifier()"
   ]
  },
  {
   "source": [
    "*Also created a confusion matrix to provide a good insight to the predictions.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20, 15), nrows = 1, ncols = 2) \n",
    "metrics.plot_confusion_matrix(lgbm_model, training_data, training_truth, cmap = plt.cm.Blues, ax = ax[0], values_format = '.6g') \n",
    "ax[0].set_title('LightGBM - Confusion Matrix of Training Data')\n",
    "metrics.plot_confusion_matrix(lgbm_model, validation_data, validation_truth, cmap = plt.cm.Blues, ax = ax[1], values_format = '.6g') \n",
    "ax[1].set_title('LightGBM - Confusion Matrix of Validation Data')\n",
    "\n",
    "# figure settings\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.4)\n",
    "fig.subplots_adjust(right=0.9)\n",
    "#fig.savefig('../plots/lgbm_cm.png', bbox_inches='tight', pad_inches=0.3)"
   ]
  },
  {
   "source": [
    "***From what we analyzed by observing the evaluation metrics (confusion matrix & support metric), the class labels of outcome are imbalanced. This could be a major contributing factor in misclassification, and this is something we hope to fix in the next milestone.***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}